{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 - Q1 [15 pts]\n",
    "\n",
    "\n",
    "\n",
    "## Important Notices\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> add any cells to this Jupyter Notebook, because that will crash the autograder.\n",
    "</div>\n",
    "\n",
    "\n",
    "All instructions, code comments, etc. in this notebook **are part of the assignment instructions**. That is, if there is instructions about completing a task in this notebook, that task is not optional.  \n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    You <strong>must</strong> implement the following functions in this notebook to receive credit.\n",
    "</div>\n",
    "\n",
    "\n",
    "`user()`\n",
    "\n",
    "`clean_data()`\n",
    "\n",
    "`common_pair()`\n",
    "\n",
    "`time_of_cheapest_fare()`\n",
    "\n",
    "`passenger_count_for_most_tip()`\n",
    "\n",
    "`day_with_traffic()`\n",
    "\n",
    "\n",
    "\n",
    "Each method will be auto-graded using different sets of parameters or data, to ensure that values are not hard-coded. You may assume we will only use your code to work with data from the NYC-TLC dataset during auto-grading. \n",
    "\n",
    "### Helper functions\n",
    "\n",
    "You are permitted to write additional helper functions, or use additional instance variables so long as the previously described functions work as required.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> remove or modify the following utility functions:\n",
    "</div>\n",
    "\n",
    "`load_data()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyspark Imports\n",
    "<span style=\"color:red\">*Please don't modify the below cell*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import hour, when, col, date_format, to_timestamp\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Spark Context\n",
    "<span style=\"color:red\">*Please don't modify the below cell*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"HW3-Q1\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to load data\n",
    "\n",
    "<span style=\"color:red\">*Please don't modify the below cell*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = sqlContext.read.option(\"header\",True).csv(\"yellow_tripdata_2019-01_short.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the functions below for this assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Update the `user()` function\n",
    "This function should return your GT username, eg: gburdell3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user():\n",
    "    \"\"\"\n",
    "    :return: string\n",
    "    your GTUsername, NOT your 9-Digit GTId  \n",
    "    \"\"\"  \n",
    "    return 'mpearl3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. [1 pts] Casting the columns into correct types\n",
    "\n",
    "To process the data accurately, cast the following columns into given data type: \n",
    "\n",
    "- `passenger_count` - integer \n",
    "- `total_amount` - float \n",
    "- `tip_amount` - float\n",
    "- `trip_distance` - float \n",
    "- `fare_amount` - float \n",
    "- `tpep_pickup_datetime` - timestamp \n",
    "- `tpep_dropoff_datetime` - timestamp \n",
    "\n",
    "All of the columns in the original data should be returned with the above columns converted to the correct data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with the all the original columns\n",
    "    '''\n",
    "    # START YOUR CODE HERE ---------   \n",
    "    from pyspark.sql.types import StructField, StructType, IntegerType, TimestampType, FloatType, StringType\n",
    "    \n",
    "    df = df.withColumn(\"passenger_count\", df[\"passenger_count\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"total_amount\", df[\"total_amount\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"tip_amount\", df[\"tip_amount\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"trip_distance\", df[\"trip_distance\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"fare_amount\", df[\"fare_amount\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"tpep_pickup_datetime\", df[\"tpep_pickup_datetime\"].cast(TimestampType()))\n",
    "    df = df.withColumn(\"tpep_dropoff_datetime\", df[\"tpep_dropoff_datetime\"].cast(TimestampType()))\n",
    "    # END YOUR CODE HERE -----------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data(df)\n",
    "#df.select(['passenger_count', 'total_amount', 'tip_amount', 'trip_distance', 'fare_amount', 'tpep_pickup_datetime', 'tpep_pickup_datetime']).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. [4 pts] What are the top 10 pickup-dropoff locations?\n",
    "\n",
    "Find the top 10 pickup-dropoff location pairs having the most number of trips (`count`). The location pairs should be ordered by `count` in descending order. If two or more pairs have the same number of trips, break the tie using the trip amount per distance travelled (`trip_rate`). Use columns `total_amount` and `trip_distance` to calculate the trip amount per distance. In certain situations, the pickup and dropoff locations may be the same.\n",
    "\n",
    "Example output showing expected formatting:\n",
    "\n",
    "```\n",
    "+------------+------------+-----+------------------+\n",
    "|PULocationID|DOLocationID|count|         trip_rate|\n",
    "+------------+------------+-----+------------------+\n",
    "|           5|           7|   24| 5.148195749283391|\n",
    "|           6|           4|   19| 1.420958193039484|\n",
    "|           3|           2|   15|9.1928382713049282|\n",
    "|           8|           8|   14|5.1029384838178493|\n",
    "|           1|           3|    9|7.4403919838271223|\n",
    "|           9|           2|    9|4.4039182884283829|\n",
    "|           5|           7|    6|  5.19283827172823|\n",
    "|           2|           1|    5| 9.233738511638532|\n",
    "|           1|           9|    3| 8.293827128489212|\n",
    "|           6|           6|    1| 4.192847382919223|\n",
    "+------------+------------+-----+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_pair(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - PULocationID\n",
    "            - DOLocationID\n",
    "            - count\n",
    "            - trip_rate\n",
    "            \n",
    "    trip_rate is the average amount (total_amount) per distance (trip_distance)\n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    from pyspark.sql import Window\n",
    "\n",
    "    partition_cols = ['PULocationID','DOLocationID']\n",
    "    group_by_result = df.groupBy(partition_cols).count()\n",
    "\n",
    "\n",
    "    df = df.withColumn(\"trip_rate_rows\",col(\"total_amount\")/col(\"trip_distance\"))\n",
    "    partition_result = df.withColumn(\"trip_rate\", sum(\"trip_rate_rows\") \\\n",
    "      .over(Window.partitionBy(*partition_cols)))\n",
    "    partition_result = partition_result.select(['PULocationID','DOLocationID','trip_rate']).distinct()\n",
    "    df_joined  = group_by_result.join(partition_result, partition_cols)\n",
    "    df = df_joined.orderBy(['count','trip_rate'], ascending=False).limit(10)\n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_pair(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c. [4 pts] When is the trip cheapest (day vs night) ?\n",
    "\n",
    "Divide each day into two parts: Day (from 9am to 8:59:59pm), and Night (from 9pm to 8:59:59am) and find the average total amount per unit distance travelled (use column `total_amount`) for both time periods. Sort the result by `trip_rate` in ascending order to find when the fare rate is cheapest.\n",
    "\n",
    "Example output showing expected formatting:\n",
    "```\n",
    "+---------+-----------------+\n",
    "|day_night|        trip_rate|\n",
    "+---------+-----------------+\n",
    "|      Day|2.391827482920123|\n",
    "|    Night|4.292818223839121|\n",
    "+---------+-----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_of_cheapest_fare(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - day_night\n",
    "            - trip_rate\n",
    "    \n",
    "    day_night will have 'Day' or 'Night' based on following conditions:\n",
    "        - From 9am to 8:59:59pm - Day\n",
    "        - From 9pm to 8:59:59am - Night\n",
    "            \n",
    "    trip_rate is the average amount (total_amount) per distance\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    from pyspark.sql.functions import date_format\n",
    "    df = df.withColumn('tpep_pickup_time', date_format('tpep_pickup_datetime', 'HH:mm:ss'))\n",
    "    day_df = df.where((df.tpep_pickup_time >= '09:00:00') & (df.tpep_pickup_time <= '20:59:59'))\n",
    "    day_df = day_df.withColumn('day_night',lit('Day'))\n",
    "\n",
    "    day_df = day_df.groupBy('day_night').agg(mean('total_amount')/mean('trip_distance').alias('trip_rate'))\n",
    "    new_cols = ['day_night', 'trip_rate']\n",
    "    day_df = day_df.toDF(*new_cols) \n",
    "\n",
    "    night_df = df.where(df.tpep_pickup_time >= '21:00:00').union(df.where(df.tpep_pickup_time <= '08:59:59'))\n",
    "    night_df = night_df.withColumn('day_night',lit('Night'))\n",
    "\n",
    "    night_df = night_df.groupBy('day_night').agg(mean('total_amount')/mean('trip_distance').alias('trip_rate'))\n",
    "    new_cols = ['day_night', 'trip_rate']\n",
    "    night_df = night_df.toDF(*new_cols) \n",
    "    df = day_df.union(night_df)\n",
    "    df = df.sort(df.trip_rate.asc())\n",
    "    \n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_of_cheapest_fare(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d. [3 pts] Which passenger group size gives the most tips?\n",
    "\n",
    "Filter the data for trips having fares (`fare_amount`) greater than $2 and the number of passengers (`passenger_count`) greater than 0. Find the average fare and tip (`tip_amount`) for all passenger group sizes and calculate the tip percent (`tip_amount * 100 / fare_amount`). Sort by the tip percent in descending order to get which group size tips most generously.\n",
    "\n",
    "Example output showing expected formatting:\n",
    "```\n",
    "+---------------+------------------+\n",
    "|passenger_count|       tip_percent|\n",
    "+---------------+------------------+\n",
    "|              4|20.129473829283771|\n",
    "|              2|16.203913838738283|\n",
    "|              3|14.283814930283822|\n",
    "|              1|13.393817383918287|\n",
    "|              6| 12.73928273747182|\n",
    "|              5|12.402938192848471|\n",
    "+---------------+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passenger_count_for_most_tip(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - passenger_count\n",
    "            - tip_percent\n",
    "            \n",
    "    trip_percent is the percent of tip out of fare_amount\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    filtered_df = df.where((df.fare_amount > 2) & (df.passenger_count > 0))\n",
    "    filtered_df = filtered_df.groupBy('passenger_count').agg((mean('tip_amount')*100)/mean('fare_amount'))\n",
    "    new_cols = ['passenger_count', 'tip_percent']\n",
    "    df = filtered_df.toDF(*new_cols) \n",
    "    df = df.sort(df.tip_percent.desc())\n",
    "    # END YOUR CODE HERE -----------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passenger_count_for_most_tip(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1e. [3 pts] Which day of week has the most traffic?\n",
    "\n",
    "Sort the days of the week based on traffic, with the day having the highest traffic on the top. You can estimate traffic on the day of the week based on the average speed of all taxi trips on that day of the week. (Speed can be calculated by using the trip time and trip distance. Make sure to print speed in distance / hour). If the `average_speed` is the same for two or more days, the days should be ordered alphabetically. A day with a low average speed indicates high levels of traffic. The average speed may be 0 indicating very high levels of traffic. Not all days of the week may be present. You should use `date_format` along with the appropriate [pattern letters](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html) to format the day of the week.\n",
    "\n",
    "Example output showing expected formatting:\n",
    "```\n",
    "+-----------+------------------+\n",
    "|day_of_week|     average_speed|\n",
    "+-----------+------------------+\n",
    "|        Sat|               0.0|\n",
    "|        Tue|               0.0|\n",
    "|        Fri|7.2938133827293934|\n",
    "|        Mon|10.123938472718228|\n",
    "+-----------+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DecimalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_with_traffic(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - day_of_week\n",
    "            - average_speed\n",
    "    \n",
    "    day_of_week should be day of week e.g.) Mon, Tue, Wed, ...\n",
    "    average_speed (miles/hour) is calculated as distance / time (in hours)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    from pyspark.sql.functions import dayofweek,date_format, unix_timestamp,format_number\n",
    "    from pyspark.sql.types import DecimalType\n",
    "    df = df.withColumn('day_of_week',date_format(col(\"tpep_pickup_datetime\"),\"EEE\"))\n",
    "    df = df.withColumn('day_of_int',dayofweek(col(\"tpep_pickup_datetime\")))\n",
    "    df = df.withColumn('trip_time',(df[\"tpep_dropoff_datetime\"].cast(\"long\") - df[\"tpep_pickup_datetime\"].cast(\"long\"))/3600)\n",
    "    #filtered_df = df.groupBy(['day_of_week','day_of_int']).agg(mean('trip_distance')/mean('trip_time'))\n",
    "    df.createOrReplaceTempView('traffic')\n",
    "    df = sqlContext.sql(\"\"\"SELECT traffic.day_of_week as day_of_week, traffic.day_of_int as day_of_int, avg(traffic.trip_distance)/ avg(traffic.trip_time) AS average_speed FROM traffic GROUP BY day_of_week,day_of_int ORDER BY average_speed, day_of_int DESC\"\"\")\n",
    "    #df = filtered_df.sort(filtered_df.day_of_int.desc()).drop(\"day_of_int\")\n",
    "    print(df.describe)\n",
    "    df = df.select(col(\"day_of_week\"),col(\"average_speed\"))\n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.describe of DataFrame[day_of_week: string, day_of_int: int, average_speed: double]>\n",
      "+-----------+------------------+\n",
      "|day_of_week|     average_speed|\n",
      "+-----------+------------------+\n",
      "|        Fri|               0.0|\n",
      "|        Wed|               0.0|\n",
      "|        Mon|1.3578837962634764|\n",
      "|        Tue| 9.383778567443406|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: string, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: int, trip_distance: float, RatecodeID: string, store_and_fwd_flag: string, PULocationID: string, DOLocationID: string, payment_type: string, fare_amount: float, extra: string, mta_tax: string, tip_amount: float, tolls_amount: string, improvement_surcharge: string, total_amount: float, congestion_surcharge: string]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#day_with_traffic(df).show()\n",
    "\n",
    "# DO NOT MODIFY\n",
    "df.select(['*'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
